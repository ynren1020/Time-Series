The future ain’t what it used to be.
YOGI BERRA, New York Yankees catcher

2.1 Introduction

Generally, we will need to distinguish between a forecast or predicted value of yt that was made at some previous time period, say, t − 𝜏, and a
fitted value of yt that has resulted from estimating the parameters in a time series model to historical data. Note that 𝜏 is the forecast lead time. 

The forecast made at time period t − 𝜏 is denoted by ŷt(t − 𝜏). There is a lot of interest in the lead − 1 forecast, which is the forecast of the observation
in period t, yt, made one period prior, ŷt(t − 1). We will denote the fitted value of yt by ŷt.

We will also be interested in analyzing forecast errors. The forecast error that results from a forecast of yt that was made at time period t − 𝜏 is
the lead −𝝉 forecast error   et(𝜏) = yt − ŷt(t − 𝜏). (2.1)
For example, the lead − 1 forecast error is  et(1) = yt − ŷt(t − 1).

The difference between the observation yt and the value obtained by fitting a time series model to the data, or a fitted value ŷt defined earlier, is called
a residual, and is denoted by et = yt − ŷt. (2.2)

The reason for this careful distinction between forecast errors and residuals is that models usually fit historical data better than they forecast. That is,
the residuals from a model-fitting process will almost always be smaller than the forecast errors that are experienced when that model is used to
forecast future observations.

2.2 GRAPHICAL DISPLAYS
2.2.1 Time Series Plots
Developing a forecasting model should always begin with graphical display and analysis of the available data.
Many of the broad general features of a time series can be seen visually.

The basic graphical display for time series data is the time series plot, a graph of yt versus the time period, t, for t = 1,2,..., T.
Features such as trend and seasonality are usually easy to see from the time series plot.

When there are two or more variables of interest, scatter plots can be useful in displaying the relationship between the variables.
The scatter plot cannot establish a causal relationship between two variables (neither can naive statistical modeling techniques,
such as regression), but it is useful in displaying how the variables have varied together in the historical data set.

2.2.2 Plotting Smoothed Data 
sometimes it is useful to overlay a smoothed version of the original data on the original time series plot to help reveal patterns
in the original data.
There are several types of data smoothers that can be employed. One of the simplest and most widely used is the orinary or simple
moving average.

A simple moving average of span N assigns weights 1/N to the most recent N observations yT, YT-1, ..., YT-N+1, and weight zero to all 
other observations. If we let MT be the moving average, then the N-span moving average at time period T is 
MT = (yT + yT-1 + ... + YT-N+1)/N = (1/N)sum(Yt) 

Clearly, as each new observation becomes available it is added into the sum from which the moving average is computed and the oldest
observation is discarded. 
The moving average has less variability than the original observations; in fact, if the variance of an individual observation yt is sigma^2,
then assuming that the observations are uncorrelated, the variance of the moving average is Var(Mt) = (sigma^2)/N

Sometimes a "centered" version of the moving average is used, such as in 
Mt = (1/(S+1))*sum(yt-i) (i from -S to S)
where the span of the centered moving average is N = 2S + 1.

The simple moving average is a linear data smoother, or a linear filter, because it replaces each observation yt with a linear combination of
the other data points that are near to it in time. The weights in the linear combination are equal, so the linear combination here is an average.

Of course, unequal weights could be used. For example, the Hanning filter is a weighted, centered moving average
Mt(H) = 0.25*yt+1 + 0.5yt + 0.25yt-1

An obvious disadvantage of a linear filter such as a moving average is that an unusual or erroneous data point or an outlier will dominate the
moving averages that contain that observation, contaminating the moving averages for a length of time equal to the span of the filter.

Odd-span moving medians (also called running medians) are an alternative to moving averages that are effective data smoothers when the time
series may be contaminated with unusual values or outliers. The moving median of span N is defined as
m[N]t = med(yt−u,… , yt,… , yt+u), (2.5)
where N = 2u + 1.

The median is the middle observation in rank order (or order of value). The moving median of span 3 is a very popular and
effective data smoother, where m[3]t = med(yt−1, yt, yt+1).

In general, a moving median will pass monotone sequences of data unchanged. It will follow a step function in the data, but it will eliminate
a spike or more persistent upset in the data that has duration of at most u consecutive observations. Moving medians can be applied more than
once if desired to obtain an even smoother series of observations.

The end values are lost when using the moving median. 
If there are a lot of observations, the information loss from the missing end values is not serious. However, if it is necessary or desirable to keep
the lengths of the original and smoothed data sets the same, a simple way to do this is to “copy on” or add back the end values from the original data.

There are also methods for smoothing the end values. Tukey (1979) is a basic reference on this subject and contains many other clever and useful
techniques for data analysis.

2.3 Nemerical Description of Time Series data 
2.3.1 Stationary Time Series
A very important type of time series is a stationary time series. A time series is said to be strictly stationary if its properties are not affected by a change in the time origin.
That is, if the joint probability distribution of the observations yt, yt+1,… , yt+n is exactly the same as the joint probability distribution of the observations yt+k, yt+k+1,… , yt+k+n then the
time series is strictly stationary. When n = 0 the stationarity assumption means that the probability distribution of yt is the same for all time periods and can be written as f(y).

Note that both time series seem to vary around a fixed level. Based on the earlier definition, this is a characteristic of stationary time series.
Stationary implies a type of statistical equilibrium or stability in the data.

2.3.2 Autocovariance and Autocorrelation Functions
The covariance between yt and its value at another time period, say, yt+k
is called the autocovariance at lag k, defined by
𝛾k = Cov(yt, yt+k) = E[(yt − 𝜇)(yt+k − 𝜇)]. (2.10)
The collection of the values of 𝛾k, k = 0, 1, 2,… is called the autocovariance function. Note that the autocovariance at lag k = 0 is just the variance of the time series; that is, 𝛾0 = 𝜎2
y ,which is constant for a stationary time series. The autocorrelation coefficient at lag k for a stationary time
series is
𝜌k = E[(yt − 𝜇)(yt+k − 𝜇)]/(√E[(yt − 𝜇)2]E[(yt+k − 𝜇)2]) = Cov(yt, yt+k)/Var(yt) = 𝛾k/𝛾0 (2.11)
The collection of the values of 𝜌k, k = 0, 1, 2,… is called the autocorrelation function (ACF). Note that by definition 𝜌0 = 1.
Furthermore, 𝜌k = 𝜌−k; that is, the ACF is symmetric around zero, so it is only necessary to compute the positive (or negative) half.

Instead of cutting off or tailing off near zero after a few lags, this sample ACF is very persistent; that is, it decays very slowly and exhibits sample autocorrelations that are still rather large
even at long lags. This behavior is characteristic of a nonstationary time series. Generally, if the sample ACF does not dampen out within about 15 to 20 lags, the time series is nonstationary.

2.3.3 The Variogram
We have discussed two techniques for determining if a time series is nonstationary, plotting a reasonable long series of the data to see if it drifts
or wanders away from its mean for long periods of time, and computing the sample ACF. However, often in practice there is no clear demarcation 
between a stationary and a nonstationary process for many real-world time series. An additional diagnostic tool that is very useful is the variogram.

Suppose that the time series observations are represented by yt. The variogram Gk measures variances of the differences between observations
that are k lags apart, relative to the variance of the differences that are one time unit apart (or at lag 1). The variogram is defined mathematically as
Gk = Var (yt+k − yt) / Var (yt+1 − yt) k = 1, 2,… (2.16)
and the values of Gk are plotted as a function of the lag k. If the time series is stationary, it turns out that Gk = (1 − 𝜌k)/(1 − 𝜌1),
but for a stationary time series 𝜌k → 0 as k increases, so when the variogram is plotted against lag k, Gk will reach an asymptote 1∕(1 − 𝜌1). However,
if the time series is nonstationary, Gk will increase monotonically.










