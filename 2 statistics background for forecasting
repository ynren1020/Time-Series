The future ain’t what it used to be.
YOGI BERRA, New York Yankees catcher

2.1 Introduction

Generally, we will need to distinguish between a forecast or predicted value of yt that was made at some previous time period, say, t − 𝜏, and a
fitted value of yt that has resulted from estimating the parameters in a time series model to historical data. Note that 𝜏 is the forecast lead time. 

The forecast made at time period t − 𝜏 is denoted by ŷt(t − 𝜏). There is a lot of interest in the lead − 1 forecast, which is the forecast of the observation
in period t, yt, made one period prior, ŷt(t − 1). We will denote the fitted value of yt by ŷt.

We will also be interested in analyzing forecast errors. The forecast error that results from a forecast of yt that was made at time period t − 𝜏 is
the lead −𝝉 forecast error   et(𝜏) = yt − ŷt(t − 𝜏). (2.1)
For example, the lead − 1 forecast error is  et(1) = yt − ŷt(t − 1).

The difference between the observation yt and the value obtained by fitting a time series model to the data, or a fitted value ŷt defined earlier, is called
a residual, and is denoted by et = yt − ŷt. (2.2)

The reason for this careful distinction between forecast errors and residuals is that models usually fit historical data better than they forecast. That is,
the residuals from a model-fitting process will almost always be smaller than the forecast errors that are experienced when that model is used to
forecast future observations.

2.2 GRAPHICAL DISPLAYS
2.2.1 Time Series Plots
Developing a forecasting model should always begin with graphical display and analysis of the available data.
Many of the broad general features of a time series can be seen visually.

The basic graphical display for time series data is the time series plot, a graph of yt versus the time period, t, for t = 1,2,..., T.
Features such as trend and seasonality are usually easy to see from the time series plot.

When there are two or more variables of interest, scatter plots can be useful in displaying the relationship between the variables.
The scatter plot cannot establish a causal relationship between two variables (neither can naive statistical modeling techniques,
such as regression), but it is useful in displaying how the variables have varied together in the historical data set.

2.2.2 Plotting Smoothed Data 
sometimes it is useful to overlay a smoothed version of the original data on the original time series plot to help reveal patterns
in the original data.
There are several types of data smoothers that can be employed. One of the simplest and most widely used is the orinary or simple
moving average.

A simple moving average of span N assigns weights 1/N to the most recent N observations yT, YT-1, ..., YT-N+1, and weight zero to all 
other observations. If we let MT be the moving average, then the N-span moving average at time period T is 
MT = (yT + yT-1 + ... + YT-N+1)/N = (1/N)sum(Yt) 

Clearly, as each new observation becomes available it is added into the sum from which the moving average is computed and the oldest
observation is discarded. 
The moving average has less variability than the original observations; in fact, if the variance of an individual observation yt is sigma^2,
then assuming that the observations are uncorrelated, the variance of the moving average is Var(Mt) = (sigma^2)/N

Sometimes a "centered" version of the moving average is used, such as in 
Mt = (1/(S+1))*sum(yt-i) (i from -S to S)
where the span of the centered moving average is N = 2S + 1.

The simple moving average is a linear data smoother, or a linear filter, because it replaces each observation yt with a linear combination of
the other data points that are near to it in time. The weights in the linear combination are equal, so the linear combination here is an average.

Of course, unequal weights could be used. For example, the Hanning filter is a weighted, centered moving average
Mt(H) = 0.25*yt+1 + 0.5yt + 0.25yt-1

An obvious disadvantage of a linear filter such as a moving average is that an unusual or erroneous data point or an outlier will dominate the
moving averages that contain that observation, contaminating the moving averages for a length of time equal to the span of the filter.

Odd-span moving medians (also called running medians) are an alternative to moving averages that are effective data smoothers when the time
series may be contaminated with unusual values or outliers. The moving median of span N is defined as
m[N]t = med(yt−u,… , yt,… , yt+u), (2.5)
where N = 2u + 1.

The median is the middle observation in rank order (or order of value). The moving median of span 3 is a very popular and
effective data smoother, where m[3]t = med(yt−1, yt, yt+1).

In general, a moving median will pass monotone sequences of data unchanged. It will follow a step function in the data, but it will eliminate
a spike or more persistent upset in the data that has duration of at most u consecutive observations. Moving medians can be applied more than
once if desired to obtain an even smoother series of observations.

The end values are lost when using the moving median. 
If there are a lot of observations, the information loss from the missing end values is not serious. However, if it is necessary or desirable to keep
the lengths of the original and smoothed data sets the same, a simple way to do this is to “copy on” or add back the end values from the original data.

There are also methods for smoothing the end values. Tukey (1979) is a basic reference on this subject and contains many other clever and useful
techniques for data analysis.

2.3 Nemerical Description of Time Series data 
2.3.1 Stationary Time Series
A very important type of time series is a stationary time series. A time series is said to be strictly stationary if its properties are not affected by a change in the time origin.
That is, if the joint probability distribution of the observations yt, yt+1,… , yt+n is exactly the same as the joint probability distribution of the observations yt+k, yt+k+1,… , yt+k+n then the
time series is strictly stationary. When n = 0 the stationarity assumption means that the probability distribution of yt is the same for all time periods and can be written as f(y).

Note that both time series seem to vary around a fixed level. Based on the earlier definition, this is a characteristic of stationary time series.
Stationary implies a type of statistical equilibrium or stability in the data.

2.3.2 Autocovariance and Autocorrelation Functions
The covariance between yt and its value at another time period, say, yt+k is called the autocovariance at lag k, defined by
𝛾k = Cov(yt, yt+k) = E[(yt − 𝜇)(yt+k − 𝜇)]. (2.10)
The collection of the values of 𝛾k, k = 0, 1, 2,… is called the autocovariance function. Note that the autocovariance at lag k = 0 is just the variance of the time series; that is, 𝛾0 = 𝜎2
y ,which is constant for a stationary time series. The autocorrelation coefficient at lag k for a stationary time series is
𝜌k = E[(yt − 𝜇)(yt+k − 𝜇)]/(√E[(yt − 𝜇)2]E[(yt+k − 𝜇)2]) = Cov(yt, yt+k)/Var(yt) = 𝛾k/𝛾0 (2.11)
The collection of the values of 𝜌k, k = 0, 1, 2,… is called the autocorrelation function (ACF). Note that by definition 𝜌0 = 1.
Furthermore, 𝜌k = 𝜌−k; that is, the ACF is symmetric around zero, so it is only necessary to compute the positive (or negative) half.

Instead of cutting off or tailing off near zero after a few lags, this sample ACF is very persistent; that is, it decays very slowly and exhibits sample autocorrelations that are still rather large
even at long lags. This behavior is characteristic of a nonstationary time series. Generally, if the sample ACF does not dampen out within about 15 to 20 lags, the time series is nonstationary.

2.3.3 The Variogram
We have discussed two techniques for determining if a time series is nonstationary, plotting a reasonable long series of the data to see if it drifts
or wanders away from its mean for long periods of time, and computing the sample ACF. However, often in practice there is no clear demarcation 
between a stationary and a nonstationary process for many real-world time series. An additional diagnostic tool that is very useful is the variogram.

Suppose that the time series observations are represented by yt. The variogram Gk measures variances of the differences between observations
that are k lags apart, relative to the variance of the differences that are one time unit apart (or at lag 1). The variogram is defined mathematically as
Gk = Var (yt+k − yt) / Var (yt+1 − yt) k = 1, 2,… (2.16)
and the values of Gk are plotted as a function of the lag k. If the time series is stationary, it turns out that Gk = (1 − 𝜌k)/(1 − 𝜌1),
but for a stationary time series 𝜌k → 0 as k increases, so when the variogram is plotted against lag k, Gk will reach an asymptote 1∕(1 − 𝜌1). However,
if the time series is nonstationary, Gk will increase monotonically.

2.4 USE OF DATA TRANSFORMATIONS AND ADJUSTMENTS
2.4.1 Transformations
Data transformations are useful in many aspects of statistical work, often for stabilizing the variance of the data. Nonconstant variance is quite common in time series data.
A very popular type of data transformation to deal with nonconstant variance is the power family of transformations.

The log transformation is used frequently in situations where the variability in the original time series increases with the average level of the series.
When the standard deviation of the original series increases linearly with the mean, the log transformation is in fact an optimal variance stabilizing transformation. The log transformation also has a very nice
physical interpretation as percentage change.
The approximate percentage change in yt can be calculated from the differences of the log-transformed time series xt ≅ 100[ln(yt) − ln(yt−1)].

2.4.2 Trend and Seasonal Adjustments
In addition to transformations, there are also several types of adjustments that are useful in time series modeling and forecasting. Two of the most
widely used are trend adjustments and seasonal adjustments. Sometimes these procedures are called trend and seasonal decomposition.

A time series that exhibits a trend is a nonstationary time series.

Modeling and forecasting of such a time series is greatly simplified if we can eliminate the trend. 
One way to do this is to fit a regression model describing the trend component to the data and then subtracting it out of the original observations, leaving a set of residuals that are free of trend.

The trend models that are usually considered are the linear trend, in which the mean of yt is expected to change linearly with time as in
E(yt) = 𝛽0 + 𝛽1t (2.19)
or as a quadratic function of time
E(yt) = 𝛽0 + 𝛽1t + 𝛽2t2 (2.20)
or even possibly as an exponential function of time such as
E(yt) = 𝛽0e𝛽1t. (2.21)
The models in Eqs. (2.19)–(2.21) are usually fit to the data by using ordinary least squares.

Another approach to removing trend is by differencing the data; that is, applying the difference operator to the original time series to obtain a new
time series, say, xt = yt − yt−1 = ∇yt, (2.22)
where ∇ is the (backward) difference operator. Another way to write the differencing operation is in terms of a backshift operator B, defined as
Byt = yt−1, so xt = (1 − B)yt = ∇yt = yt − yt−1 (2.23) with ∇ = (1 − B). Differencing can be performed successively if necessary until the trend is removed.

Differencing has two advantages relative to fitting a trend model to the data. First, it does not require estimation of any parameters, so it is a more
parsimonious (i.e., simpler) approach; and second, model fitting assumes that the trend is fixed throughout the time series history and will remain
so in the (at least immediate) future. In other words, the trend component, once estimated, is assumed to be deterministic. Differencing can allow
the trend component to change through time. The first difference accounts for a trend that impacts the change in the mean of the time series, the
second difference accounts for changes in the slope of the time series, and so forth. Usually, one or two differences are all that is required in practice
to remove an underlying trend in the data.

Seasonal, or both trend and seasonal, components are present in many time series. Differencing can also be used to eliminate seasonality. Define
a lag—d seasonal difference operator as ∇dyt = (1 − B^d) = yt − yt−d. (2.26) For example, if we had monthly data with an annual season (a very common
situation), we would likely use d = 12, so the seasonally differenced data would be ∇12yt = (1 − B12)yt = yt − yt−12.
When both trend and seasonal components are simultaneously present, we can sequentially difference to eliminate these effects. That is, 
first seasonally difference to remove the seasonal component and then difference one or more times using the regular difference operator to remove the trend.

There is also a “classical” approach to decomposition of a time series into trend and seasonal components. The general mathematical model for this decomposition is
yt = f(St, Tt, 𝜀t), where St is the seasonal component, Tt is the trend effect (sometimes called the trend-cycle effect), and 𝜀t is the random error component. There are
usually two forms for the function f ; an additive model yt = St + Tt + 𝜀t and a multiplicative model yt = StTt𝜀t.

The additive model is appropriate if the magnitude (amplitude) of the seasonal variation does not vary with the level of the series, while the
multiplicative version is more appropriate if the amplitude of the seasonal fluctuations increases or decreases with the average level of the time series.

2.5 GENERAL APPROACH TO TIME SERIES MODELING AND FORECASTING
The basic steps in modeling and forecasting a time series are as follows:
1. Plot the time series and determine its basic features, such as whether trends or seasonal behavior or both are present. Look for possible
outliers or any indication that the time series has changed with respect to its basic features (such as trends or seasonality) over the time period
history.

2. Eliminate any trend or seasonal components, either by differencing or by fitting an appropriate model to the data. Also consider using data
transformations, particularly if the variability in the time series seems to be proportional to the average level of the series. The objective of
these operations is to produce a set of stationary residuals.

3. Develop a forecasting model for the residuals. It is not unusual to find that there are several plausible models, and additional analysis will
have to be performed to determine the best one to deploy. Sometimes potential models can be eliminated on the basis of their fit to the
historical data. It is unlikely that a model that fits poorly will produce good forecasts.

4. Validate the performance of the model (or models) from the previous step. This will probably involve some type of split-sample or crossvalidation procedure. The objective of this step is to select a model
to use in forecasting.

5. Also of interest are the differences between the original time series yt and the values that would be forecast by the model on the original
scale. To forecast values on the scale of the original time series yt, reverse the transformations and any differencing adjustments made
to remove trends or seasonal effects.

6. For forecasts of future values in period T + 𝜏 on the original scale, if a transformation was used, say, xt = ln yt, then the forecast made
at the end of period T for T + 𝜏 would be obtained by reversing the transformation.

7. If prediction intervals are desired for the forecast (and we recommend doing this), construct prediction intervals for the residuals and
then reverse the transformations made to produce the residuals as described earlier.

8. Develop and implement a procedure for monitoring the forecast to ensure that deterioration in performance will be detected reasonably
quickly. Forecast monitoring is usually done by evaluating the stream of forecast errors that are experienced.

Note: Why is it important to have stationary residuals by doing data transformation before fitting models for time series data
Ensuring that residuals are stationary in time series analysis is crucial for several reasons:
Model Assumptions: Many time series models, such as ARIMA, assume that the data are stationary. Stationarity implies that the statistical properties of the series (mean, variance, autocorrelation) do not change over time. If the residuals are non-stationary, it indicates that the model has not captured all the underlying patterns in the data.
Predictive Accuracy: A model with stationary residuals is generally more reliable for forecasting because it indicates that the model has effectively captured the signal in the data, leaving only random noise. Non-stationary residuals suggest that important structures or trends in the data have been missed.
Statistical Inference: Stationary residuals allow for more valid statistical inference, as many inferential techniques rely on the assumption of stationarity. This affects the confidence intervals and hypothesis tests related to model parameters.
Error Independence: Stationarity often implies that residuals are independent over time. Independence is a key assumption for many statistical tests and indicates that the model has adequately explained the temporal dependencies in the data.
Improved Diagnostics: When residuals are stationary, diagnostic checks like autocorrelation functions (ACF) and partial autocorrelation functions (PACF) are more interpretable, allowing for better model refinement and validation.
Transforming data to achieve stationarity might involve differencing, detrending, or applying logarithmic, square root, or other transformations. These adjustments help ensure that the model accurately represents the underlying process and can provide reliable predictions.

2.6 EVALUATING AND MONITORING FORECASTING MODEL PERFORMANCE
2.6.1 Forecasting Model Evaluation
We now consider how to evaluate the performance of a forecasting technique for a particular time series or application. It is important to carefully
define the meaning of performance. It is tempting to evaluate performance on the basis of the fit of the forecasting or time series model to historical data. 
There are many statistical measures that describe how well a model fits a given sample of data, and several of these will be described in subsequent chapters. 
This goodness-of-fit approach often uses the residuals and does not really reflect the capability of the forecasting technique to
successfully predict future observations. The user of the forecasts is very concerned about the accuracy of future forecasts, not model goodness of
fit, so it is important to evaluate this aspect of any recommended technique. Sometimes forecast accuracy is called “out-of-sample” forecast error, to
distinguish it from the residuals that arise from a model-fitting process.

Measure of forecast accuracy should always be evaluated as part of a model validation effort. When more than one forecasting technique seems
reasonable for a particular application, these forecast accuracy measures can also be used to discriminate between competing models.

If a time series consists of uncorrelated observations and has constant variance, we say that it is white noise. If, in addition, the observations
in this time series are normally distributed, the time series is Gaussian white noise. Ideally, forecast errors are Gaussian white noise.

2.6.2 Choosing Between Competing Models





